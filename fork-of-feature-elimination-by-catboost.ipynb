{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"}],"dockerImageVersionId":30580,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Based on https://www.kaggle.com/code/lblhandsome/optiver-robust-best-single-model.\n\nOriginal version: 147 feats, 1 Fold, LGBM on GPU (Valid MAE 5.88949 | LB 5.3455).\n\n*This solution includes some changes in feature engeneering taken from top scoring notebooks by now*.\n\n**Version 1 of this Notebook**: **124** feats, 1 Fold, CatBoost on CPU (Valid MAE **5.89079** | LB **5.3490**) as a baseline.\n\nAs mentioned in the [discussion](https://www.kaggle.com/competitions/optiver-trading-at-the-close/discussion/454190): \"To make the model robust to a private leaderboard, we need to come up with a way to remove unnecessary features\".\n\n**Version 2 of this Notebook**: Performing Recursive Feature Elimination based on SHAP values to compare with the results from Verison 1. Reduced feats number: **100**, 1 Fold, CatBoost on CPU (Valid MAE **5.88783** | LB **5.3469**).","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport warnings\nfrom warnings import simplefilter\nfrom itertools import combinations\n\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom catboost import CatBoostRegressor, EShapCalcType, EFeaturesSelectionAlgorithm\nfrom sklearn.metrics import mean_absolute_error\n\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T12:57:38.346726Z","iopub.execute_input":"2023-12-20T12:57:38.347141Z","iopub.status.idle":"2023-12-20T12:57:38.354024Z","shell.execute_reply.started":"2023-12-20T12:57:38.347108Z","shell.execute_reply":"2023-12-20T12:57:38.352638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!jupyter nbextension enable --py widgetsnbextension","metadata":{"execution":{"iopub.status.busy":"2023-12-20T12:57:41.780869Z","iopub.execute_input":"2023-12-20T12:57:41.781372Z","iopub.status.idle":"2023-12-20T12:57:43.307122Z","shell.execute_reply.started":"2023-12-20T12:57:41.781327Z","shell.execute_reply":"2023-12-20T12:57:43.305546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up parameters\nis_offline = False    # Flag for online/offline mode\nis_train = True    # Flag for training mode\nis_infer = True    # Flag for inference mode\nsplit_day = 435    # Split day for time series data","metadata":{"execution":{"iopub.status.busy":"2023-12-20T12:57:44.096628Z","iopub.execute_input":"2023-12-20T12:57:44.097077Z","iopub.status.idle":"2023-12-20T12:57:44.103103Z","shell.execute_reply.started":"2023-12-20T12:57:44.097045Z","shell.execute_reply":"2023-12-20T12:57:44.101792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\ndf = df.dropna(subset=[\"target\"])\ndf.reset_index(drop=True, inplace=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-20T12:58:00.682288Z","iopub.execute_input":"2023-12-20T12:58:00.682701Z","iopub.status.idle":"2023-12-20T12:58:14.062408Z","shell.execute_reply.started":"2023-12-20T12:58:00.682667Z","shell.execute_reply":"2023-12-20T12:58:14.061214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.fillna(method='ffill')\ndel(df['far_price'])\ndel(df['near_price'])","metadata":{"execution":{"iopub.status.busy":"2023-12-20T12:59:32.492793Z","iopub.execute_input":"2023-12-20T12:59:32.493197Z","iopub.status.idle":"2023-12-20T12:59:33.19347Z","shell.execute_reply.started":"2023-12-20T12:59:32.493164Z","shell.execute_reply":"2023-12-20T12:59:33.192296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-20T12:59:35.721223Z","iopub.execute_input":"2023-12-20T12:59:35.721608Z","iopub.status.idle":"2023-12-20T12:59:36.055939Z","shell.execute_reply.started":"2023-12-20T12:59:35.721578Z","shell.execute_reply":"2023-12-20T12:59:36.054972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=0):\n    \"\"\"\n    Iterate through all numeric columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \"\"\"\n    # Calculate the initial memory usage of the DataFrame\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                # Check if the column's data type is a float\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n                    \n    if verbose:\n        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n        end_mem = df.memory_usage().sum() / 1024**2\n        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n        decrease = 100 * (start_mem - end_mem) / start_mem\n        logger.info(f\"Decreased by {decrease:.2f}%\")\n\n    return df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-20T13:03:23.162659Z","iopub.execute_input":"2023-12-20T13:03:23.16311Z","iopub.status.idle":"2023-12-20T13:03:23.175963Z","shell.execute_reply.started":"2023-12-20T13:03:23.163076Z","shell.execute_reply":"2023-12-20T13:03:23.17477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Numba for just-in-time (JIT) compilation and parallel processing\nfrom numba import njit, prange\n\n# Function to compute triplet imbalance in parallel using Numba\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    # Loop through all combinations of triplets\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        \n        # Loop through rows of the DataFrame\n        for j in range(num_rows):\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n            \n            # Prevent division by zero\n            if mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n\n    return imbalance_features\n\n\n# Function to calculate triplet imbalance for given price data and a DataFrame\ndef calculate_triplet_imbalance_numba(price, df):\n    # Convert DataFrame to numpy array for Numba compatibility\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n\n    # Calculate the triplet imbalance using the Numba-optimized function\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n\n    # Create a DataFrame from the results\n    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n\n    return features","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-20T13:03:25.704402Z","iopub.execute_input":"2023-12-20T13:03:25.704996Z","iopub.status.idle":"2023-12-20T13:03:27.258862Z","shell.execute_reply.started":"2023-12-20T13:03:25.704944Z","shell.execute_reply":"2023-12-20T13:03:27.257496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to generate imbalance features\ndef imbalance_features(df):\n    # Define lists of price and size-related column names\n    prices = [\"reference_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n\n    # V1 features\n    # Calculate various features using Pandas eval function\n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"mid_price\"] = df.eval(\"ask_price + bid_price\")/2\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"imbalance_size-matched_size\")/df.eval(\"matched_size+imbalance_size\")\n    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n    \n    # Create features for pairwise price imbalances\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n        \n    # V2 features\n    # Calculate additional features\n    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n    \n    \n    # V3 features\n    # Calculate shifted and return features for specific columns\n    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n        for window in [1, 2, 3, 4, 8, 12]:\n            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n    \n    # Calculate diff features for specific columns\n    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size',\n                'market_urgency', 'imbalance_momentum', 'size_imbalance']:\n        for window in [1, 2, 3, 4, 8, 12]:\n            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n    # Replace infinite values with 0\n    return df.replace([np.inf, -np.inf], 0)\n\n\ndef numba_imb_features(df):\n    prices = [\"reference_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n    \n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n        \n    # Calculate triplet imbalance features using the Numba-optimized function\n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values\n    return df\n\n\n# Function to generate time and stock-related features\ndef other_features(df):\n    df[\"dow\"] = df[\"date_id\"] % 5                 # Day of the week\n    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n\n    # Map global features to the DataFrame\n    for key, value in global_stock_id_feats.items():\n        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n\n    return df\n\n\n# Function to generate all features by combining imbalance and other features\ndef generate_all_features(df):\n    # Select relevant columns for feature generation\n    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n    df = df[cols]\n    \n    # Generate imbalance features\n    df = imbalance_features(df)\n    df = numba_imb_features(df)\n    # Generate time and stock-related features\n    df = other_features(df)\n    gc.collect()\n    \n    # Select and return the generated features\n    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n    \n    return df[feature_name]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-12-20T13:06:12.652092Z","iopub.execute_input":"2023-12-20T13:06:12.652523Z","iopub.status.idle":"2023-12-20T13:06:12.673988Z","shell.execute_reply.started":"2023-12-20T13:06:12.652494Z","shell.execute_reply":"2023-12-20T13:06:12.672191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if the code is running in offline or online mode\nif is_offline:\n    # In offline mode, split the data into training and validation sets based on the split_day\n    df_train = df[df[\"date_id\"] <= split_day]\n    df_valid = df[df[\"date_id\"] > split_day]\n    print(\"Offline mode\")\n    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\nelse:\n    # In online mode, use the entire dataset for training\n    df_train = df\n    print(\"Online mode\")","metadata":{"execution":{"iopub.status.busy":"2023-12-20T13:06:18.257003Z","iopub.execute_input":"2023-12-20T13:06:18.257387Z","iopub.status.idle":"2023-12-20T13:06:18.26616Z","shell.execute_reply.started":"2023-12-20T13:06:18.257356Z","shell.execute_reply":"2023-12-20T13:06:18.263612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nif is_train:\n    global_stock_id_feats = {\n        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n    }\n    if is_offline:\n        df_train_feats = generate_all_features(df_train)\n        print(\"Build Train Feats Finished.\")\n        df_valid_feats = generate_all_features(df_valid)\n        print(\"Build Valid Feats Finished.\")\n        df_valid_feats = reduce_mem_usage(df_valid_feats)\n    else:\n        df_train_feats = generate_all_features(df_train)\n        print(\"Build Online Train Feats Finished.\")\n\n    df_train_feats = reduce_mem_usage(df_train_feats)\n    \nfeature_name = list(df_train_feats.columns)\nprint(f\"Feature length = {len(feature_name)}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-20T13:06:24.870314Z","iopub.execute_input":"2023-12-20T13:06:24.870798Z","iopub.status.idle":"2023-12-20T13:07:45.407234Z","shell.execute_reply.started":"2023-12-20T13:06:24.870762Z","shell.execute_reply":"2023-12-20T13:07:45.406386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CatBoost Feature Selection Tutorial can be found [here](https://github.com/catboost/tutorials/blob/master/feature_selection/select_features_tutorial.ipynb).\n\nIt contains examples, parameters description and valuable explanations.","metadata":{}},{"cell_type":"code","source":"%%time\n# Train procedure\nif is_train:\n    offline_split = df_train['date_id']>(split_day - 45)\n    df_offline_train = df_train_feats[~offline_split]\n    df_offline_valid = df_train_feats[offline_split]\n    df_offline_train_target = df_train['target'][~offline_split]\n    df_offline_valid_target = df_train['target'][offline_split]\n    \n    ctb_params = dict(iterations=700,\n                             learning_rate=0.1,\n                             depth=12,\n                             eval_metric='RMSE',\n                             random_seed = 23,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = 75,\n                             od_wait=100)\n\n    print(\"Feature Elimination Performing.\")\n    ctb_model = CatBoostRegressor(**ctb_params)\n    summary = ctb_model.select_features(\n        df_offline_train[feature_name], df_offline_train_target,\n        eval_set=[(df_offline_valid[feature_name], df_offline_valid_target)],\n        features_for_select=feature_name,\n        num_features_to_select=len(feature_name)-42,    # Dropping from 142 to 100\n        steps=3,\n        algorithm=EFeaturesSelectionAlgorithm.RecursiveByShapValues,\n        shap_calc_type=EShapCalcType.Regular,\n        train_final_model=False,\n        plot=True,\n    )\n    \n    print(\"Valid Model Training on Selected Features Subset.\")\n    ctb_model = CatBoostRegressor(**ctb_params)\n    ctb_model.fit(\n        df_offline_train[summary['selected_features_names']], df_offline_train_target,\n        eval_set=[(df_offline_valid[summary['selected_features_names']], df_offline_valid_target)],\n        use_best_model=True,\n    )\n    \n    del df_offline_train, df_offline_valid, df_offline_train_target, df_offline_valid_target\n    gc.collect()\n    \n    df_train_target = df_train[\"target\"]\n    print(\"Infer Model Training on Selected Features Subset.\")\n    infer_params = ctb_params.copy()\n    # CatBoost train best with Valid number of iterations\n    infer_params[\"iterations\"] = ctb_model.best_iteration_\n    infer_ctb_model = CatBoostRegressor(**infer_params)\n    infer_ctb_model.fit(df_train_feats[summary['selected_features_names']], df_train_target)\n    print(\"Infer Model Training on Selected Features Subset Complete.\")\n    \n    if is_offline:   \n        # Offline predictions\n        df_valid_target = df_valid[\"target\"]\n        offline_predictions = infer_ctb_model.predict(df_valid_feats[summary['selected_features_names']])\n        offline_score = mean_absolute_error(offline_predictions, df_valid_target)\n        print(f\"Offline Score {np.round(offline_score, 4)}\")\n        \n    feat_importances = ctb_model.get_feature_importance(prettified=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-20T12:57:33.875424Z","iopub.status.idle":"2023-12-20T12:57:33.875942Z","shell.execute_reply.started":"2023-12-20T12:57:33.875731Z","shell.execute_reply":"2023-12-20T12:57:33.875756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary['eliminated_features_names']","metadata":{"execution":{"iopub.status.busy":"2023-12-20T12:57:33.877381Z","iopub.status.idle":"2023-12-20T12:57:33.878182Z","shell.execute_reply.started":"2023-12-20T12:57:33.877962Z","shell.execute_reply":"2023-12-20T12:57:33.877993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 10))\nsns.barplot(x=\"Importances\", y=\"Feature Id\", data=feat_importances.loc[:30, :])\nplt.title('CatBoost top features importance:')\n\nplt.figure(figsize=(12, 10))\nsns.barplot(x=\"Importances\", y=\"Feature Id\", data=feat_importances.tail(30))\nplt.title('CatBoost lower features importance:')","metadata":{"execution":{"iopub.status.busy":"2023-12-20T12:57:33.879691Z","iopub.status.idle":"2023-12-20T12:57:33.880157Z","shell.execute_reply.started":"2023-12-20T12:57:33.879957Z","shell.execute_reply":"2023-12-20T12:57:33.879984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Infer procedure\ndef zero_sum(prices, volumes):\n    std_error = np.sqrt(volumes)\n    step = np.sum(prices)/np.sum(std_error)\n    out = prices-std_error*step\n    return out\n\nif is_infer:\n    import optiver2023\n    env = optiver2023.make_env()\n    iter_test = env.iter_test()\n    counter = 0\n    y_min, y_max = -64, 64\n    qps, predictions = [], []\n    cache = pd.DataFrame()\n    \n    for (test, revealed_targets, sample_prediction) in iter_test:\n        now_time = time.time()\n        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n        if counter > 0:\n            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n        feat = generate_all_features(cache)[-len(test):]\n        \n        # Selected features subset\n        feat = feat[summary['selected_features_names']]\n        \n        ctb_predictions = infer_ctb_model.predict(feat)\n        ctb_predictions = zero_sum(ctb_predictions, test['bid_size'] + test['ask_size'])\n        clipped_predictions = np.clip(ctb_predictions, y_min, y_max)\n        sample_prediction['target'] = clipped_predictions\n        env.predict(sample_prediction)\n        counter += 1\n        qps.append(time.time() - now_time)\n        if counter % 10 == 0:\n            print(counter, 'qps:', np.mean(qps))\n           \n    time_cost = 1.146 * np.mean(qps)\n    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")","metadata":{"execution":{"iopub.status.busy":"2023-12-20T12:57:33.881696Z","iopub.status.idle":"2023-12-20T12:57:33.882137Z","shell.execute_reply.started":"2023-12-20T12:57:33.881945Z","shell.execute_reply":"2023-12-20T12:57:33.881964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}